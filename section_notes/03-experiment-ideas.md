

# Experiment Ideas

## Overview

Following Computer Science-inspired research methodology, these experiments test core assumptions about AI-assisted research platforms. Each experiment follows the assumption + hypothesis paradigm, where we identify implicit assumptions in existing research tools and propose alternatives that could reshape how researchers work.

**Core Research Question**: Can an AI co-scientist fundamentally improve research productivity by challenging assumptions about how literature review, collaboration, and experiment tracking should work?

## Planned Experiments

### Experiment 1: AI-Accelerated Literature Review Efficiency
- **Objective**: Test the assumption that literature reviews must be time-intensive manual processes
- **Core Assumption Being Tested**: Traditional keyword-based search is the optimal method for discovering relevant research
- **Hypothesis**: AI-powered semantic literature discovery can reduce literature review time by 80% while improving coverage quality
- **Independent Variables**: 
  - Search method (traditional keyword vs. AI semantic search)
  - Researcher experience level (novice vs. expert)
- **Dependent Variables**:
  - Time to complete comprehensive literature review
  - Number of relevant papers discovered
  - Coverage quality score (expert evaluation)
  - Researcher satisfaction ratings
- **Methodology**: 
  1. Recruit 40 researchers across 4 domains (CS, Biology, Psychology, Economics)
  2. Random assignment to control (traditional tools) vs. treatment (Oslo AI assistant)
  3. Standardized literature review tasks with expert-validated ground truth
  4. Measure completion time, coverage quality, and satisfaction
- **Expected Outcomes**: 70-85% time reduction with equal or better quality
- **Success Metrics**: 
  - Time reduction ≥ 70%
  - Quality score ≥ control group
  - Satisfaction score ≥ 7/10
- **Validity Threats & Mitigations**:
  - Selection bias: Random sampling from multiple institutions
  - Learning effects: Counterbalanced design
  - Domain specificity: Test across multiple academic fields

### Experiment 2: Collaborative Research Hypothesis Generation
- **Objective**: Test whether AI can identify novel research directions by connecting disparate literature
- **Core Assumption Being Tested**: Human researchers are optimal at identifying cross-domain research opportunities
- **Hypothesis**: AI analysis of literature patterns can generate testable hypotheses that human experts rate as novel and valuable
- **Independent Variables**:
  - Hypothesis generation method (human-only vs. AI-assisted vs. AI-generated)
  - Domain complexity (single-domain vs. cross-domain)
- **Dependent Variables**:
  - Novelty ratings by domain experts
  - Feasibility ratings
  - Potential impact scores
  - Time to generate hypothesis set
- **Methodology**:
  1. Collect 100 recent high-impact papers across 5 research domains
  2. Generate hypotheses using three methods: human experts only, human + AI, AI-generated
  3. Expert panel blind evaluation of novelty, feasibility, and impact
  4. Track time and resource requirements for each method
- **Expected Outcomes**: AI-assisted methods will generate higher-rated hypotheses in less time
- **Success Metrics**:
  - Novelty score improvement ≥ 25%
  - Time reduction ≥ 60%
  - Expert preference for AI-assisted hypotheses ≥ 70%

### Experiment 3: Reproducibility Through AI Experiment Tracking
- **Objective**: Test whether automated AI experiment documentation improves research reproducibility
- **Core Assumption Being Tested**: Manual documentation is sufficient for research reproducibility
- **Hypothesis**: AI-powered automatic experiment tracking will increase successful replication rates from current 40% to 80%+
- **Independent Variables**:
  - Documentation method (manual vs. AI-assisted tracking)
  - Experiment complexity (simple vs. complex protocols)
- **Dependent Variables**:
  - Successful replication rate
  - Time to understand and replicate
  - Documentation completeness score
  - Replicator confidence ratings
- **Methodology**:
  1. Partner with 20 research labs conducting computational experiments
  2. Random assignment to manual documentation vs. Oslo AI tracking
  3. After 6 months, attempt replication with independent researchers
  4. Measure replication success, time, and satisfaction
- **Expected Outcomes**: Dramatic improvement in replication success rates
- **Success Metrics**:
  - Replication success rate ≥ 75%
  - Time to replicate reduced by ≥ 50%
  - Documentation completeness score ≥ 90%

### Experiment 4: AI-Powered Peer Review Quality Enhancement  
- **Objective**: Test whether AI can improve peer review consistency and quality
- **Core Assumption Being Tested**: Human-only peer review provides optimal manuscript evaluation
- **Hypothesis**: AI-assisted peer review will reduce review variance while maintaining quality, addressing the "Reviewer 2" problem
- **Independent Variables**:
  - Review type (traditional vs. AI-assisted)
  - Reviewer expertise level (junior vs. senior)
- **Dependent Variables**:
  - Inter-reviewer agreement scores
  - Review quality ratings (author feedback)
  - Time to complete review
  - Identification of critical issues
- **Methodology**:
  1. Partner with journals for double-blind study
  2. Same manuscripts reviewed by traditional and AI-assisted reviewers
  3. Compare consistency, thoroughness, and author satisfaction
  4. Track review completion times and quality metrics
- **Expected Outcomes**: Higher consistency and faster reviews without quality loss
- **Success Metrics**:
  - Inter-reviewer agreement increase ≥ 40%
  - Review completion time reduction ≥ 30%
  - Author satisfaction improvement ≥ 25%

## Experimental Design Principles

Each experiment follows rigorous CS evaluation standards:
1. **Clear Hypothesis Structure**: Testing specific assumptions about research processes
2. **Controlled Variables**: Systematic manipulation of key factors
3. **Objective Measurements**: Quantifiable metrics over subjective assessments
4. **Statistical Power**: Sample sizes calculated for 80% power at α = 0.05
5. **Validity Controls**: Multiple threat mitigations and replication provisions
6. **Ethical Considerations**: IRB approval for human subjects research

## Timeline

**Phase 1 (Months 1-2): Setup & Preparation**
- IRB approvals and ethics reviews
- Participant recruitment and onboarding
- System development and testing
- Baseline measurements

**Phase 2 (Months 3-8): Experiment Execution**
- Month 3-4: Experiment 1 (Literature Review Efficiency)
- Month 4-5: Experiment 2 (Hypothesis Generation) 
- Month 5-7: Experiment 3 (Reproducibility Tracking)
- Month 6-8: Experiment 4 (Peer Review Enhancement)

**Phase 3 (Months 9-10): Analysis & Reporting**
- Data analysis and statistical testing
- Result interpretation and implications
- Paper writing and submission preparation

## Resource Requirements

- **Personnel**: 2 postdocs, 4 graduate students, 1 software engineer
- **Participants**: ~200 researchers across experiments
- **Technology**: Cloud computing resources, AI model access
- **Budget**: Estimated $150K for participant compensation and infrastructure
- **Partnerships**: 3-5 academic journals, 20+ research laboratories

## Risk Mitigation

**High-Risk Assumptions**:
1. **Researcher adoption**: Mitigation through extensive user testing and feedback
2. **AI capability limits**: Conservative effect size estimates and fallback methods
3. **Institutional resistance**: Partner engagement and incentive alignment
4. **Measurement validity**: Expert validation of all assessment instruments

